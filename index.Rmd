---
title: "Estadística"
subtitle: "Aplicaciones de los momentos: entropía diferencial 2"
author: "Julio César Ramírez Pacheco"
date: "13/11/2023"
output:
  rmdformats::material:
    highlight: kate
    cards: false
---


```{r knitr_init, echo=FALSE, message=FALSE, warning=FALSE}
library(highcharter)
```



# Entropía diferencial

Sea $f(x)$ la densidad de probabilidad de un experimento aleatorio $\mathbb{E}$. Recordemos que la entropía de la función $f(x)$ (llamada entropía diferencial) está dada por la siguiente expresión:
$$
h(X) = -\int_{-\infty}^{+\infty}{f(x)\log(f(x))}.
$$

La entropía diferencial es pues, la entropía de Shannon para distribuciones que corresponden a variables aleatorias continuas, por ejemplo para la variable aleatoria uniforme, como se vió en la tarea pasada, la entropía tiene la siguiente relación densidad-entropía:
$$
h(f(x)=\frac{1}{b-a}) = \ln(b-a)
$$

y por lo tanto se puede notar que para el caso de la distribución uniforme al incrementar la varianza (cuando $a$ incrementa), se incrementa la entropía. La siguiente figura muestra lo anterior.

```{r eval=TRUE}
a          <- 0
b          <- seq(2,8, length=20)               # Variamos b
entropy    <- log(b-a) 
hc <- highchart() %>% 
  hc_add_series(cbind(b,entropy), name="UniformRV_entropy") %>%   hc_add_theme(hc_theme_smpl()) %>% 
  hc_title(text="Variacion de la entropia con la Varianza") %>%   hc_subtitle(text="Teoria de la informacion") %>%
  hc_xAxis(title=list(text="Valores de b")) %>%          hc_yAxis(title=list(text="Entropia de la funcion"))
hc

```

## Ejercicios

Investigar la relación varianza-entropia para las siguientes variables aleatorias continuas:

- Rayleigh
- Normal
- Exponencial
- Cauchy
- Laplace
- Logística
- Triangular

Para la variable aleatoria triangular, ?Existe una relación entre su moda y su entropía?

Nota: Para responder adecuadamente los anteriores cuestionamientos es necesario investigar las entropías de las variables aleatorias así como los valores de sus varianzas. De igual forma es necesario conocer el funcionamiento del paquete de `R` llamado `highcharter`.


# Entropía de Shannon discreta

La entropía mide el grado de complejidad de una variable aleatoria descrita por medio de su PDF o bién mediante su PMF. Para el caso discreto, la ecuación entrópica de Shannon está dada por:
$$
H(p) = -\sum_{k}{p_k \log(p_k)}
$$

Para la variable aleatoria Binomial, la PMF está dada por:
$$
\mbox{Pr}\{X=k\} = {n\choose k} p^k(1-p)^{n-k}
$$
y por lo tanto, la relación entre la entropía y la probabilidad $p$ está dada empíricamente como:

```{r eval=TRUE}
n          <- 20
x          <- 0:20
p          <- seq(0,1, length=20)
entropies  <- numeric(20)
for(i in 1:length(p))
{
  densities     <- dbinom(x,n,p[i])
  entropies[i]  <- -1*sum(densities*log(densities))
  
}
theoretical <- 0.5*log(2*pi*n*exp(1)*p*(1-p))
hc <- highchart() %>% 
  hc_add_series(cbind(p,entropies), name="BinomialRV_empirical") %>%  hc_add_series(cbind(p,theoretical), name="BinomialRV_theoretical") %>%  hc_add_theme(hc_theme_smpl()) %>% 
  hc_title(text="Variacion de la entropia contra p") %>%   hc_subtitle(text="Teoria de la informacion") %>%
  hc_xAxis(title=list(text="Valores de probabilidad p")) %>%          hc_yAxis(title=list(text="Entropia de la funcion"))
hc

```

## Ejercicios

Replicar el mismo procedimiento anterior para las siguientes variables aleatorias discretas:

- Binomial negativa.
- Geométrica.
- Poisson.
- Hipergeométrica.